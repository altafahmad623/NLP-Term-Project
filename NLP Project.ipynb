{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07293473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline Joint Model\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "from itertools import combinations\n",
    "from typing import Iterator, List, Mapping, Union, Optional, Set\n",
    "import logging as log\n",
    "import abc\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack, pad_packed_sequence as unpack\n",
    "from featurize_data import matres_label_map, tbd_label_map\n",
    "from functools import partial\n",
    "from sklearn.model_selection import KFold, ParameterGrid, train_test_split\n",
    "from utils import ClassificationReport\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"Puts data, and lengths into a packed_padded_sequence then returns                           \n",
    "    the packed_padded_sequence and the labels. Set use_lengths to True\n",
    "    to use this collate function.                                                                                                                                                                                                    \n",
    "        \"\"\"\n",
    "    if len(batch) >= 1:\n",
    "\n",
    "        bs  = list(zip(*[ex for ex in sorted(batch, key=lambda x: x[2].shape[0], reverse=True)]))\n",
    "        \n",
    "        max_len, n_fts = bs[2][0].shape\n",
    "        lengths = [x.shape[0] for x in bs[2]]\n",
    "        \n",
    "        sents = [torch.cat((torch.FloatTensor(s), torch.zeros(max_len - s.shape[0], n_fts)), 0) \n",
    "                 if s.shape[0] != max_len else torch.FloatTensor(s) for s in bs[2]]\n",
    "        sents = torch.stack(sents, 0)\n",
    "        \n",
    "        all_key_ent = [list(zip(*key_ent)) for key_ent in bs[3]]\n",
    "\n",
    "        keys = [[(bs[0][i], k) for k in v[0]] for i, v in enumerate(all_key_ent)]\n",
    "\n",
    "        ents = [v[1] for v in all_key_ent]\n",
    "        ents = [torch.cat((torch.LongTensor(s).unsqueeze(1), torch.zeros(max_len - len(s), 1, dtype=torch.long)), 0)\n",
    "                if len(s) != max_len else torch.LongTensor(s).unsqueeze(1) for s in ents]\n",
    "        ents = torch.stack(ents, 0).squeeze(2)\n",
    "        \n",
    "    return bs[0], bs[1], sents, keys, ents, bs[4], bs[5], lengths\n",
    "\n",
    "class EventDataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data_dir, data_split):\n",
    "        'Initialization'\n",
    "        # load data\n",
    "        with open(data_dir + data_split + '.pickle', 'rb') as handle:\n",
    "            self.data = pickle.load(handle)\n",
    "            self.data = list(self.data.values())\n",
    "        handle.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        sample = self.data[idx]\n",
    "        doc_id = sample['doc_id']\n",
    "        context_id = sample['context_id']\n",
    "        context = sample['context']\n",
    "        rels = sample['rels']\n",
    "\n",
    "        return doc_id, context_id, context[0], context[1], context[2],  rels\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    'NN Archi'\n",
    "    def __init__(self, args):\n",
    "        \n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        self.hid_size = args.hid\n",
    "        self.batch_size = args.batch\n",
    "        self.num_layers = args.num_layers\n",
    "        self.num_classes = len(args.label_to_id)\n",
    "        self.num_ent_classes = 2\n",
    "\n",
    "        self.dropout = nn.Dropout(p=args.dropout)\n",
    "        #shared lstm \n",
    "        self.lstm = nn.LSTM(768, self.hid_size, self.num_layers, bias = False, bidirectional=True)\n",
    "\n",
    "        #relation\n",
    "        self.linear1 = nn.Linear(self.hid_size*4+args.n_fts, self.hid_size)\n",
    "        self.linear2 = nn.Linear(self.hid_size, self.num_classes)\n",
    "\n",
    "        #entity\n",
    "        self.linear1_ent = nn.Linear(self.hid_size*2, int(self.hid_size / 2))\n",
    "        self.linear2_ent = nn.Linear(int(self.hid_size / 2), self.num_ent_classes)\n",
    "\n",
    "        self.act = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.softmax_ent = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, sents, lengths, fts = [], rel_idxs=[], lidx_start=[], lidx_end=[], ridx_start=[], \n",
    "                ridx_end=[], pred_ind=True, flip=False, causal=False, token_type_ids=None, task='relation'):\n",
    "\n",
    "        batch_size = sents.size(0)\n",
    "        out = self.dropout(sents)\n",
    "        out, _ = self.lstm(pack(out, lengths, batch_first=True))\n",
    "        out, _ = unpack(out, batch_first = True)\n",
    "        # entity\n",
    "        if task == 'entity':\n",
    "            out_ent = self.linear1_ent(self.dropout(out))\n",
    "            out_ent = self.act(out_ent)\n",
    "            out_ent = self.linear2_ent(out_ent)\n",
    "            prob_ent = self.softmax_ent(out_ent)\n",
    "            return out_ent, prob_ent\n",
    "        \n",
    "        # relaton\n",
    "        if task == 'relation':\n",
    "            \n",
    "            ltar_f = torch.cat([out[b, lidx_start[b][r], :self.hid_size].unsqueeze(0) for b,r in rel_idxs], dim=0)\n",
    "            ltar_b = torch.cat([out[b, lidx_end[b][r], self.hid_size:].unsqueeze(0) for b,r in rel_idxs], dim=0)\n",
    "            rtar_f = torch.cat([out[b, ridx_start[b][r], :self.hid_size].unsqueeze(0) for b,r in rel_idxs], dim=0)\n",
    "            rtar_b = torch.cat([out[b, ridx_end[b][r], self.hid_size:].unsqueeze(0) for b,r in rel_idxs], dim=0)\n",
    "        \n",
    "            out = self.dropout(torch.cat((ltar_f, ltar_b, rtar_f, rtar_b), dim=1))\n",
    "            out = torch.cat((out, fts), dim=1)\n",
    "                                                                                               \n",
    "            out = self.linear1(out)\n",
    "            out = self.act(out)\n",
    "            out = self.dropout(out)\n",
    "            out = self.linear2(out)\n",
    "            prob = self.softmax(out)            \n",
    "            return out, prob\n",
    "\n",
    "@dataclass()\n",
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNClassifier, self).__init__()\n",
    "\n",
    "    def predict(self, model, data, args, test=False, gold=True, model_r=None):\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()                                                                           \n",
    "        count = 1\n",
    "        labels, probs, losses_t, losses_e = [], [], [], []\n",
    "        pred_inds, docs, pairs = [], [], []\n",
    "        nopred_rels = []\n",
    "\n",
    "        ent_pred_map, ent_label_map = {}, {}\n",
    "        rd_pred_map, rd_label_map = {}, {}\n",
    "\n",
    "        for doc_id, context_id, sents, ent_keys, ents, poss, rels, lengths in data:\n",
    "\n",
    "            if args.cuda:\n",
    "                sents = sents.cuda()\n",
    "                ents = ents.cuda()\n",
    "            out_e, prob_e = model(sents, lengths, task='entity')\n",
    "\n",
    "            labels_r, fts, rel_idxs, doc, pair, lidx_start, lidx_end, ridx_start, ridx_end, nopred_rel = self.construct_relations(prob_e, lengths, rels, list(doc_id), poss, gold=gold)\n",
    "            \n",
    "            nopred_rels.extend(nopred_rel)\n",
    "\n",
    "            if rel_idxs: \n",
    "                docs.extend(doc)\n",
    "                pairs.extend(pair)\n",
    "\n",
    "                if args.cuda:\n",
    "                    labels_r = labels_r.cuda()\n",
    "                    fts = fts.cuda()\n",
    "\n",
    "                if model_r:\n",
    "                    model_r.eval()\n",
    "                    out_r, prob_r = model_r(sents, lengths, fts=fts, rel_idxs=rel_idxs, lidx_start=lidx_start,\n",
    "                                            lidx_end=lidx_end, ridx_start=ridx_start, ridx_end=ridx_end)\n",
    "                else:\n",
    "                    out_r, prob_r = model(sents, lengths, fts=fts, rel_idxs=rel_idxs, lidx_start=lidx_start,\n",
    "                                          lidx_end=lidx_end, ridx_start=ridx_start, ridx_end=ridx_end)\n",
    "                loss_r = criterion(out_r, labels_r)\n",
    "                predicted = (prob_r.data.max(1)[1]).long().view(-1)\n",
    "\n",
    "                if args.cuda:\n",
    "                    loss_r = loss_r.cpu()\n",
    "                    prob_r = prob_r.cpu()\n",
    "                    labels_r = labels_r.cpu()\n",
    "                \n",
    "                losses_t.append(loss_r.data.numpy())\n",
    "                probs.append(prob_r)\n",
    "                labels.append(labels_r)\n",
    "\n",
    "            ent_pred, ent_label, ent_prob, ent_key, ent_pos = [], [], [], [], []\n",
    "            for i,l in enumerate(lengths):\n",
    "                ent_pred.append(out_e[i, :l])\n",
    "                ent_prob.append(prob_e[i, :l])\n",
    "                ent_label.append(ents[i, :l])\n",
    "                assert len(ent_keys[i]) == l\n",
    "                ent_key.extend(ent_keys[i])\n",
    "                ent_pos.extend([p for p in poss[i]])\n",
    "\n",
    "            ent_pred = torch.cat(ent_pred, 0)\n",
    "            ent_label = torch.cat(ent_label, 0)\n",
    "            ent_probs = torch.cat(ent_prob, 0)\n",
    "            \n",
    "            assert ent_pred.size(0) == ent_label.size(0)\n",
    "            assert ent_pred.size(0) == len(ent_key)\n",
    "\n",
    "            loss_e = criterion(ent_pred, ent_label)\n",
    "            losses_e.append(loss_e.cpu().data.numpy())\n",
    "                \n",
    "            ent_label = ent_label.tolist()\n",
    "            \n",
    "            for i, v in enumerate(ent_key):\n",
    "                label_e = ent_label[i]\n",
    "                prob_e = ent_probs[i]\n",
    "\n",
    "                if v in [\"[SEP]\", \"[CLS]\"]:\n",
    "                    assert ent_pos[i] in [\"[SEP]\", \"[CLS]\"]\n",
    "\n",
    "                if v not in ent_pred_map:\n",
    "                    ent_pred_map[v] = [prob_e.tolist()[1]]\n",
    "                    ent_label_map[v] = (label_e, ent_pos[i])\n",
    "                else:\n",
    "                    ent_pred_map[v].append(prob_e.tolist()[1])\n",
    "                    assert ent_label_map[v][0] == label_e\n",
    "                    assert ent_label_map[v][1] == ent_pos[i]\n",
    "            \n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                print(\"finished evaluating %s samples\" % (count * args.batch))\n",
    "\n",
    "        probs = torch.cat(probs,dim=0)\n",
    "        labels = torch.cat(labels,dim=0)\n",
    "\n",
    "        assert labels.size(0) == probs.size(0)\n",
    "\n",
    "        ent_pred_map_agg = {k:1 if np.mean(v) > 0.5 else 0 for k,v in ent_pred_map.items()}\n",
    "\n",
    "        n_correct = 0\n",
    "        n_pred = 0\n",
    "            \n",
    "        pos_keys = OrderedDict([(k, v) for k, v in ent_label_map.items() if v[0]==1])\n",
    "        n_true = len(pos_keys)\n",
    "\n",
    "        for k,v in ent_label_map.items():\n",
    "            if ent_pred_map_agg[k] == 1:\n",
    "                n_pred += 1\n",
    "            if ent_pred_map_agg[k] == 1 and ent_label_map[k][0] == 1:\n",
    "                n_correct += 1\n",
    "\n",
    "        print(n_pred, n_true, n_correct)\n",
    "\n",
    "        def safe_division(numr, denr, on_err=0.0):\n",
    "            return on_err if denr == 0.0 else float(numr) / float(denr)\n",
    "\n",
    "        precision = safe_division(n_correct, n_pred)\n",
    "        recall = safe_division(n_correct, n_true)\n",
    "        f1_score = safe_division(2.0 * precision * recall, precision + recall)\n",
    "        \n",
    "        print(\"Evaluation temporal relation loss: %.4f\" % np.mean(losses_t))\n",
    "        print(\"Evaluation temporal entity loss: %.4f; F1: %.4f\" % (np.mean(losses_e), f1_score))\n",
    "\n",
    "        if test:\n",
    "            return probs.data, np.mean(losses_t), labels, docs, pairs, f1_score, nopred_rels\n",
    "        else:\n",
    "            return probs.data, np.mean(losses_t), labels, docs, pairs, n_pred, n_true, n_correct, nopred_rels\n",
    "\n",
    "    def construct_relations(self, ent_probs, lengths, rels, doc, poss, gold=True, train=True):\n",
    "        \n",
    "        nopred_rels = []\n",
    "\n",
    "        if gold:\n",
    "            pred_rels = rels\n",
    "\n",
    "        else:\n",
    "            def _is_gold(pred_span, gold_rel_span):\n",
    "                return ((gold_rel_span[0] <= pred_span <= gold_rel_span[1]))\n",
    "                \n",
    "            batch_size = ent_probs.size(0)\n",
    "            ent_probs = ent_probs.cpu()\n",
    "            \n",
    "            ent_locs = [[x for x in (ent_probs[b,:, 1] > 0.5).nonzero().view(-1).tolist() \n",
    "                         if x < lengths[b]] for b in range(batch_size)]\n",
    "\n",
    "            rel_locs = [list(combinations(el, 2)) for el in ent_locs]\n",
    "\n",
    "            pred_rels = []\n",
    "            totl = 0\n",
    "            neg_counter = min([int(x[0][1:]) for rel in rels for x in rel])\n",
    "            \n",
    "            for i, rl in enumerate(rel_locs):\n",
    "                temp_rels, temp_ids = [], []\n",
    "                for r in rl:\n",
    "                    sent_segs = len([x for x in poss[i] if x == '[SEP]'])\n",
    "                    in_seg = [x for x in poss[i][r[0] : r[1]] if x == '[SEP]']\n",
    "                    if (sent_segs > 1) and (len(in_seg) == 0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        totl += 1\n",
    "                        gold_match = [x for x in rels[i] if _is_gold(r[0], x[5][:2]) and _is_gold(r[1], x[5][2:])]\n",
    "                        if len(gold_match) > 0 and gold_match[0][0] not in temp_ids:\n",
    "                            temp_rels.append(gold_match[0])\n",
    "                            temp_ids.append(gold_match[0][0])\n",
    "                        else:\n",
    "                            neg_id = 'N%s' % neg_counter\n",
    "                            left_match = [x for x in rels[i] if _is_gold(r[0], x[5][:2])]\n",
    "                            right_match = [x for x in rels[i] if _is_gold(r[1], x[5][2:])]\n",
    "                            left_id = left_match[0][1][0] if len(left_match) > 0 else ('e%s' % (neg_counter + 10000))\n",
    "                            right_id = right_match[0][1][1] if len(right_match) > 0 else ('e%s' % (neg_counter + 20000))\n",
    "                            a_rel = (neg_id, (left_id, right_id), self._label_to_id['NONE'],\n",
    "                                     [float(r[1] - r[0])], False, (r[0], r[0], r[1], r[1]), True)\n",
    "                            temp_rels.append(a_rel)\n",
    "                            neg_counter += 1\n",
    "                nopred_rels.extend([x[2] for x in rels[i] if x[0] not in [tr[0] for tr in temp_rels]])\n",
    "                pred_rels.append(temp_rels)\n",
    "\n",
    "        docs, pairs = [], []\n",
    "        rel_idxs, lidx_start, lidx_end, ridx_start, ridx_end = [],[],[],[],[]\n",
    "        for i, rel in enumerate(pred_rels):\n",
    "            rel_idxs.extend([(i, ii) for ii, _ in enumerate(rel)])\n",
    "            lidx_start.append([x[5][0] for x in rel])\n",
    "            lidx_end.append([x[5][1] for x in rel])\n",
    "            ridx_start.append([x[5][2] for x in rel])\n",
    "            ridx_end.append([x[5][3] for x in rel])\n",
    "            pairs.extend([x[1] for x in rel])\n",
    "            docs.extend([doc[i] for _ in rel])\n",
    "        assert len(docs) == len(pairs)\n",
    "            \n",
    "        rels = [x for rel in pred_rels for x in rel]\n",
    "        if rels == []:\n",
    "            labels = torch.FloatTensor([])\n",
    "            fts = torch.FloatTensor([])\n",
    "        else:\n",
    "            labels = torch.LongTensor([x[2] for x in rels])\n",
    "            fts = torch.cat([torch.FloatTensor(x[3]) for x in rels]).unsqueeze(1)\n",
    "        \n",
    "        return labels, fts, rel_idxs, docs, pairs, lidx_start, lidx_end, ridx_start, ridx_end, nopred_rels\n",
    "\n",
    "    def _train(self, train_data, eval_data, pos_emb, args):\n",
    "\n",
    "        model = BertClassifier(args)\n",
    "\n",
    "        if args.cuda:\n",
    "            print(\"using cuda device: %s\" % torch.cuda.current_device())\n",
    "            assert torch.cuda.is_available()\n",
    "            model.cuda()\n",
    "\n",
    "\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "        criterion_e = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if args.data_type in ['tbd']:\n",
    "            weights = torch.FloatTensor([1.0, 1.0, 1.0, args.uw, args.uw, args.uw, 1.0])\n",
    "  \n",
    "        else:\n",
    "            weights = torch.FloatTensor([1.0, 1.0, 1.0, args.uw, 1.0])\n",
    "            \n",
    "        if args.cuda:\n",
    "            weights = weights.cuda()\n",
    "        \n",
    "        criterion_r = nn.CrossEntropyLoss(weight=weights) \n",
    "        losses = [] \n",
    "\n",
    "        sents, poss, ftss, labels = [], [], [], []                                                                            \n",
    "        if args.load_model == True:\n",
    "            checkpoint = torch.load(args.ilp_dir + args.entity_model_file)\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            epoch = checkpoint['epoch']\n",
    "            best_eval_f1 = checkpoint['f1']\n",
    "            print(\"Local best eval f1 is: %s\" % best_eval_f1)\n",
    "                                                                     \n",
    "        best_eval_f1 = 0.0 \n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            print(\"Training Epoch #%s...\" % epoch)\n",
    "            model.train()\n",
    "            count = 1\n",
    "\n",
    "            loss_hist_t, loss_hist_e = [], []\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            gold = False if epoch > args.pipe_epoch else True\n",
    "            for doc_id, context_id, sents, keys, ents, poss, rels, lengths in train_data:\n",
    "\n",
    "                if args.cuda:\n",
    "                    sents = sents.cuda()\n",
    "                    ents = ents.cuda()\n",
    "\n",
    "                model.zero_grad() \n",
    "                     \n",
    "                out_e, prob_e = model(sents, lengths, task='entity')                     \n",
    "\n",
    "                labels_r, fts, rel_idxs, _, _, lidx_start, lidx_end, ridx_start, ridx_end, _ = self.construct_relations(prob_e, lengths, rels, list(doc_id), poss, gold=gold)\n",
    "\n",
    "                if args.cuda:\n",
    "                    labels_r = labels_r.cuda()\n",
    "                    fts = fts.cuda()\n",
    "\n",
    "                ent_pred, ent_label = [], []\n",
    "                \n",
    "                for i,l in enumerate(lengths):\n",
    "                    ent_pred.append(out_e[i, :l])\n",
    "                    ent_label.append(ents[i, :l])\n",
    "\n",
    "                ent_pred = torch.cat(ent_pred, 0)\n",
    "                ent_label = torch.cat(ent_label, 0)\n",
    "                \n",
    "                assert ent_pred.size(0) == ent_label.size(0)\n",
    "\n",
    "                loss_e = criterion_e(ent_pred, ent_label)\n",
    "                \n",
    "                loss_r = 0\n",
    "                if rel_idxs:\n",
    "                    out_r, prob_r = model(sents, lengths, fts=fts, rel_idxs=rel_idxs, lidx_start=lidx_start, \n",
    "                                          lidx_end=lidx_end, ridx_start=ridx_start, ridx_end=ridx_end)                    \n",
    "                    loss_r = criterion_r(out_r, labels_r)\n",
    "                \n",
    "                loss = args.relation_weight * loss_r + args.entity_weight * loss_e\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step() \n",
    "            \n",
    "                if args.cuda:\n",
    "                    if loss_r != 0:\n",
    "                        loss_hist_t.append(loss_r.data.cpu().numpy())\n",
    "                    loss_hist_e.append(loss_e.data.cpu().numpy())\n",
    "                else:\n",
    "                    if loss_r != 0:\n",
    "                        loss_hist_t.append(loss_r.data.numpy())                                        \n",
    "                    loss_hist_e.append(loss_e.data.numpy())\n",
    "\n",
    "                if count % 100 == 0:                                                                                     \n",
    "                    print(\"trained %s samples\" % (count * args.batch))\n",
    "                    print(\"Temporal loss is %.4f\" % np.mean(loss_hist_t))\n",
    "                    print(\"Entity loss is %.4f\" % np.mean(loss_hist_e))\n",
    "                    print(\"%.4f seconds elapsed\" % (time.time() - start_time))                                                              \n",
    "                count += 1\n",
    " \n",
    "            if len(eval_data) > 0:\n",
    "\n",
    "                eval_gold = gold\n",
    "                eval_preds, eval_loss, eval_labels,  _, _, ent_pred, ent_true, ent_corr, nopred_rels = self.predict(model, eval_data, args, gold=eval_gold)\n",
    "                pred_labels = eval_preds.max(1)[1].long().view(-1)\n",
    "                assert eval_labels.size() == pred_labels.size()\n",
    "                                \n",
    "                eval_correct = (pred_labels == eval_labels).sum()\n",
    "                eval_acc =  float(eval_correct) / float(len(eval_labels))\n",
    "\n",
    "                pred_labels = list(pred_labels.numpy())\n",
    "                eval_labels = list(eval_labels.numpy())\n",
    "\n",
    "                if not eval_gold:\n",
    "                    print(len(nopred_rels))\n",
    "                    pred_labels.extend([self._label_to_id['NONE'] for _ in nopred_rels])\n",
    "                    eval_labels.extend(nopred_rels)\n",
    "\n",
    "                if args.data_type in ['red', 'caters']:\n",
    "                    pred_labels = [pred_labels[k] if v == 1 else self._label_to_id['NONE'] for k,v in enumerate(pred_inds)]\n",
    "\n",
    "                eval_f1 = self.weighted_f1(pred_labels, eval_labels, ent_corr, ent_pred, ent_true, \n",
    "                                           args.relation_weight, args.entity_weight)\n",
    "\n",
    "                if eval_f1 > best_eval_f1 and (epoch > args.pipe_epoch or args.pipe_epoch >= 1000):\n",
    "                    best_eval_f1 = eval_f1\n",
    "                    self.model = copy.deepcopy(model)\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                print(\"Eval Loss: %.4f; Eval F1: %.4f\" % (eval_loss, eval_f1))\n",
    "\n",
    "        print(\"Final Eval F1: %.4f at Epoch %s\" % (best_eval_f1, best_epoch))\n",
    "\n",
    "        if len(eval_data) == 0 or args.load_model:\n",
    "            self.model = copy.deepcopy(model)\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if args.save_model == True:\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'args': args,\n",
    "                        'state_dict': self.model.cpu().state_dict(),\n",
    "                        'f1': best_eval_f1,\n",
    "                        'optimizer' : optimizer.state_dict()\n",
    "                    }, \"%s%s.pth.tar\" % (args.ilp_dir, args.save_stamp))\n",
    "        \n",
    "        return best_eval_f1, best_epoch\n",
    "                          \n",
    "    def train_epoch(self, train_data, dev_data, args, test_data = None):\n",
    "\n",
    "        if args.data_type == \"matres\":\n",
    "            label_map = matres_label_map\n",
    "        if args.data_type == \"tbd\":\n",
    "            label_map = tbd_label_map\n",
    "\n",
    "        all_labels = list(OrderedDict.fromkeys(label_map.values()))\n",
    "        all_labels.append('NONE')\n",
    "\n",
    "        self._label_to_id = OrderedDict([(all_labels[l],l) for l in range(len(all_labels))])\n",
    "        self._id_to_label = OrderedDict([(l,all_labels[l]) for l in range(len(all_labels))])\n",
    "\n",
    "        args.label_to_id = self._label_to_id\n",
    "\n",
    "        pos_emb= np.zeros((len(args.pos2idx) + 1, len(args.pos2idx) + 1))\n",
    "        for i in range(pos_emb.shape[0]):\n",
    "            pos_emb[i, i] = 1.0\n",
    "\n",
    "        best_f1, best_epoch = self._train(train_data, dev_data, pos_emb, args)\n",
    "        print(\"Final Dev F1: %.4f\" % best_f1)\n",
    "        return best_f1, best_epoch\n",
    "\n",
    "    def weighted_f1(self, pred_labels, true_labels, ent_corr, ent_pred, ent_true, rw=0.0, ew=0.0):\n",
    "        def safe_division(numr, denr, on_err=0.0):\n",
    "            return on_err if denr == 0.0 else numr / denr\n",
    "\n",
    "        assert len(pred_labels) == len(true_labels)\n",
    "\n",
    "        weighted_f1_scores = {}\n",
    "        if 'NONE' in self._label_to_id.keys():\n",
    "            num_tests = len([x for x in true_labels if x != self._label_to_id['NONE']])\n",
    "        else:\n",
    "            num_tests = len([x for x in true_labels])\n",
    "\n",
    "        print(\"Total pos samples for eval: %s\" % num_tests)\n",
    "        total_true = Counter(true_labels)\n",
    "        total_pred = Counter(pred_labels)\n",
    "\n",
    "        labels = list(self._id_to_label.keys())\n",
    "\n",
    "        n_correct = 0\n",
    "        n_true = 0\n",
    "        n_pred = 0\n",
    "\n",
    "        if rw > 0:\n",
    "            exclude_labels = ['VAGUE', 'NONE'] if len(self._label_to_id) == 5 else ['NONE']\n",
    "\n",
    "            for label in labels:\n",
    "                if self._id_to_label[label] not in exclude_labels:\n",
    "\n",
    "                    true_count = total_true.get(label, 0)\n",
    "                    pred_count = total_pred.get(label, 0)\n",
    "\n",
    "                    n_true += true_count\n",
    "                    n_pred += pred_count\n",
    "\n",
    "                    correct_count = len([l for l in range(len(pred_labels))\n",
    "                                         if pred_labels[l] == true_labels[l] and pred_labels[l] == label])\n",
    "                    n_correct += correct_count\n",
    "        if ew > 0:\n",
    "            n_correct += ent_corr\n",
    "            n_pred += ent_pred\n",
    "            n_true += ent_true\n",
    "\n",
    "        precision = safe_division(n_correct, n_pred)\n",
    "        recall = safe_division(n_correct, n_true)\n",
    "        f1_score = safe_division(2.0 * precision * recall, precision + recall)\n",
    "        print(\"Final Precision: %.4f\\tRecall: %.4f\\tF1: %.4f\" % (precision, recall, f1_score))\n",
    "\n",
    "        return(f1_score)\n",
    "\n",
    "class EventEvaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self, test_data, args):\n",
    "        if args.model == \"singletask/pipeline\":\n",
    "            model_r = BertClassifier(args)\n",
    "            if args.cuda:\n",
    "                print(\"using cuda device: %s\" % torch.cuda.current_device())\n",
    "                assert torch.cuda.is_available()\n",
    "                model_r.cuda()\n",
    "            checkpoint = torch.load(args.ilp_dir + args.relation_model_file)\n",
    "            model_r.load_state_dict(checkpoint['state_dict'])\n",
    "            preds, loss, true_labels, docs, pairs, ent_f1, nopred_rels = self.model.predict(self.model.model, \n",
    "                                                                                            test_data, \n",
    "                                                                                            args, \n",
    "                                                                                            test = True, \n",
    "                                                                                            gold = False,\n",
    "                                                                                            model_r = model_r)\n",
    "        else:\n",
    "            preds, loss, true_labels, docs, pairs, ent_f1, nopred_rels \\\n",
    "                = self.model.predict(self.model.model, test_data, args, test = True, gold = args.eval_gold)\n",
    "            \n",
    "        preds = (preds.max(1)[1]).long().view(-1)\n",
    "\n",
    "        pred_labels = preds.numpy().tolist()\n",
    "        true_labels = true_labels.tolist()\n",
    "        if not args.eval_gold:\n",
    "            print(len(nopred_rels))\n",
    "            pred_labels.extend([self.model._label_to_id['NONE'] for _ in nopred_rels])\n",
    "            true_labels.extend(nopred_rels)\n",
    "\n",
    "        rel_f1 = self.model.weighted_f1(pred_labels, true_labels, 0, 0, 0, rw=1.0)\n",
    "\n",
    "        pred_labels = [self.model._id_to_label[x] for x in pred_labels]\n",
    "        true_labels = [self.model._id_to_label[x] for x in true_labels]\n",
    "\n",
    "        print(len(pred_labels), len(true_labels), len(pairs), len(docs))\n",
    "        out = ClassificationReport(args.model, true_labels, pred_labels)\n",
    "        print(out)\n",
    "        print(\"F1 Excluding Vague: %.4f\" % rel_f1)\n",
    "        return rel_f1, ent_f1\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    data_dir = args.data_dir\n",
    "    opt_args = {}\n",
    "\n",
    "    params = {'batch_size': args.batch,\n",
    "              'shuffle': False,\n",
    "              'collate_fn': pad_collate}\n",
    "\n",
    "    type_dir = \"/all_context/\"\n",
    "    test_data = EventDataset(args.data_dir + type_dir, \"test\")\n",
    "    test_generator = data.DataLoader(test_data, **params)\n",
    "\n",
    "    train_data = EventDataset(args.data_dir + type_dir, \"train\")\n",
    "    train_generator = data.DataLoader(train_data, **params)\n",
    "\n",
    "    dev_data = EventDataset(args.data_dir + type_dir, \"dev\")\n",
    "    dev_generator = data.DataLoader(dev_data, **params)    \n",
    "    \n",
    "    model = NNClassifier()\n",
    "    print(f\"======={args.model}=====\\n\")\n",
    "    best_f1, best_epoch = model.train_epoch(train_generator, dev_generator, args)\n",
    "    evaluator = EventEvaluator(model)\n",
    "    rel_f1, ent_f1 = evaluator.evaluate(test_generator, args)\n",
    "    print(rel_f1, ent_f1)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('-data_dir', type=str, default = '../data')\n",
    "    p.add_argument('-other_dir', type=str, default = '../other')\n",
    "    p.add_argument('-model', type=str, default='multitask/pipeline')#, 'multitask/gold', 'multitask/pipeline'\n",
    "    p.add_argument('-emb', type=int, default=100)\n",
    "    p.add_argument('-hid', type=int, default=100)\n",
    "    p.add_argument('-num_layers', type=int, default=1)\n",
    "    p.add_argument('-batch', type=int, default=2)\n",
    "    p.add_argument('-data_type', type=str, default=\"matres\")\n",
    "    p.add_argument('-epochs', type=int, default=5)\n",
    "    p.add_argument('-pipe_epoch', type=int, default=1000) # 1000: no pipeline training; otherwise <= epochs \n",
    "    p.add_argument('-seed', type=int, default=123)\n",
    "    p.add_argument('-lr', type=float, default=0.0005)\n",
    "    p.add_argument('-num_classes', type=int, default=2) # get updated in main()\n",
    "    p.add_argument('-dropout', type=float, default=0.1)\n",
    "    p.add_argument('-ngbrs', type=int, default = 15)                                   \n",
    "    p.add_argument('-pos2idx', type=dict, default = {})\n",
    "    p.add_argument('-w2i', type=OrderedDict)\n",
    "    p.add_argument('-glove', type=OrderedDict)\n",
    "    p.add_argument('-cuda', action='store_true')\n",
    "    p.add_argument('-refit_all', type=bool, default=False)\n",
    "    p.add_argument('-uw', type=float, default=1.0)\n",
    "    p.add_argument('-params', type=dict, default={})\n",
    "    p.add_argument('-n_splits', type=int, default=5)\n",
    "    p.add_argument('-pred_win', type=int, default=200)\n",
    "    p.add_argument('-n_fts', type=int, default=1)\n",
    "    p.add_argument('-relation_weight', type=float, default=1.0)\n",
    "    p.add_argument('-entity_weight', type=float, default=15.0)\n",
    "    p.add_argument('-save_model', type=bool, default=False)\n",
    "    p.add_argument('-save_stamp', type=str, default=\"matres_entity_best\")\n",
    "    p.add_argument('-entity_model_file', type=str, default=\"\")\n",
    "    p.add_argument('-relation_model_file', type=str, default=\"\")\n",
    "    p.add_argument('-load_model', type=bool, default=False)\n",
    "    p.add_argument('-bert_config', type=dict, default={})\n",
    "    p.add_argument('-fine_tune', type=bool, default=False)\n",
    "    p.add_argument('-eval_gold',type=bool, default=True)\n",
    "    args = p.parse_args()\n",
    "    args.save_stamp = \"%s_hid%s_dropout%s_ew%s\" % (args.save_stamp, args.hid, args.dropout, args.entity_weight)\n",
    "\n",
    "    args.eval_list = []\n",
    "    args.data_dir += args.data_type\n",
    "\n",
    "    tags = open(args.other_dir + \"/pos_tags.txt\")\n",
    "    pos2idx = {}\n",
    "    idx = 0\n",
    "    for tag in tags:\n",
    "        tag = tag.strip()\n",
    "        pos2idx[tag] = idx\n",
    "        idx += 1\n",
    "    args.pos2idx = pos2idx\n",
    "    \n",
    "    args.idx2pos = {v+1:k for k,v in pos2idx.items()}\n",
    "\n",
    "    print(args.hid, args.dropout, args.entity_weight, args.relation_weight)\n",
    "    main(args)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
